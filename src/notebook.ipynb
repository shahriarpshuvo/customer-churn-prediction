{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8033ca1d",
      "metadata": {
        "id": "aacc414e",
        "language": "markdown"
      },
      "source": [
        "# Notebook\n",
        "\n",
        "A Jupyter notebook to preprocess datasets for sentiment analysis and churn prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d367b82",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "Import necessary libraries and download NLTK resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b6515146",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/shahriarpshuvo/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/shahriarpshuvo/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/shahriarpshuvo/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63b461a5",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 2. Define Utility Functions\n",
        "\n",
        "Define text cleaning and lemmatization functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "858fad5f",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def lemmatize_text(text, lemmatizer, stop_words):\n",
        "    words = text.split()\n",
        "    lemmatized = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
        "    return ' '.join(lemmatized)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731003d6",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 3. Preprocess Amazon Fine Food Reviews\n",
        "\n",
        "Clean, label, and split the Amazon reviews dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ed752efe",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def preprocess_amazon_reviews(input_path, output_dir):\n",
        "    df = pd.read_csv(input_path)\n",
        "    df = df[['Text','Score']].dropna()\n",
        "    df = df[df.Score.isin([1,2,3,4,5])]\n",
        "    df['label'] = df.Score.apply(lambda x: 'negative' if x<=2 else 'neutral' if x==3 else 'positive')\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    df['clean_text'] = df.Text.apply(clean_text)\n",
        "    df['clean_text'] = df.clean_text.apply(lambda x: lemmatize_text(x, lemmatizer, stop_words))\n",
        "\n",
        "    df_final = df[['clean_text','label']]\n",
        "\n",
        "    train, vt = train_test_split(df_final, test_size=0.3, stratify=df_final.label, random_state=42)\n",
        "    val, test = train_test_split(vt, test_size=0.5, stratify=vt.label, random_state=42)\n",
        "\n",
        "    train.to_csv(os.path.join(output_dir,'sentiment_train.csv'), index=False)\n",
        "    val.to_csv(os.path.join(output_dir,'sentiment_val.csv'), index=False)\n",
        "    test.to_csv(os.path.join(output_dir,'sentiment_test.csv'), index=False)\n",
        "\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8fd383",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4. Preprocess Telco Customer Churn\n",
        "\n",
        "Clean, encode, and split the Telco churn dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b8e414eb",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def preprocess_telco_churn(input_path, output_dir):\n",
        "    df = pd.read_csv(input_path)\n",
        "    df.dropna(inplace=True)\n",
        "    df['Churn'] = df.Churn.map({'Yes':1,'No':0})\n",
        "\n",
        "    df = df.drop(columns=['customerID'])\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    df = pd.get_dummies(df, columns=cat_cols)\n",
        "\n",
        "    train, vt = train_test_split(df, test_size=0.3, stratify=df.Churn, random_state=42)\n",
        "    val, test = train_test_split(vt, test_size=0.5, stratify=vt.Churn, random_state=42)\n",
        "\n",
        "    train.to_csv(os.path.join(output_dir,'churn_train.csv'), index=False)\n",
        "    val.to_csv(os.path.join(output_dir,'churn_val.csv'), index=False)\n",
        "    test.to_csv(os.path.join(output_dir,'churn_test.csv'), index=False)\n",
        "\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee53b2d",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 5. Run Preprocessing Scripts\n",
        "\n",
        "Execute preprocessing for both datasets and save outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "2c091252",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Preprocessing complete\n"
          ]
        }
      ],
      "source": [
        "data_dir = '../data'\n",
        "train_s, val_s, test_s = preprocess_amazon_reviews(f\"{data_dir}/amazon_reviews.csv\", data_dir)\n",
        "train_c, val_c, test_c = preprocess_telco_churn(f\"{data_dir}/telco_churn.csv\", data_dir)\n",
        "print('✅ Preprocessing complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0022421",
      "metadata": {},
      "source": [
        "## 6. Train Churn Prediction Models\n",
        "\n",
        "Train Random Forest and Logistic Regression on the churn dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b63e0aea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and labels\n",
        "Xc_train = train_c.drop(columns=['Churn'])\n",
        "yc_train = train_c['Churn']\n",
        "Xc_val   = val_c.drop(columns=['Churn'])\n",
        "yc_val   = val_c['Churn']\n",
        "Xc_test  = test_c.drop(columns=['Churn'])\n",
        "yc_test  = test_c['Churn']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler().fit(Xc_train)\n",
        "Xc_train_s = scaler.transform(Xc_train)\n",
        "Xc_val_s   = scaler.transform(Xc_val)\n",
        "Xc_test_s  = scaler.transform(Xc_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c59de7",
      "metadata": {},
      "source": [
        "### 6.1 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3dd3dbf6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       776\n",
            "           1       0.66      0.46      0.54       281\n",
            "\n",
            "    accuracy                           0.79      1057\n",
            "   macro avg       0.74      0.69      0.70      1057\n",
            "weighted avg       0.78      0.79      0.78      1057\n",
            "\n",
            "RF AUC-ROC: 0.8160\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../models/saved_scaler.pkl']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(Xc_train_s, yc_train)\n",
        "pred_rf = rf.predict(Xc_test_s)\n",
        "prob_rf = rf.predict_proba(Xc_test_s)[:,1]\n",
        "print('Random Forest Classification Report:')\n",
        "print(classification_report(yc_test, pred_rf))\n",
        "print(f'RF AUC-ROC: {roc_auc_score(yc_test, prob_rf):.4f}')\n",
        "joblib.dump(rf, '../models/saved_churn_rf.pkl')\n",
        "joblib.dump(scaler, '../models/saved_scaler.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3736a75",
      "metadata": {},
      "source": [
        "### 6.2 Logistics Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "9680c598",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       776\n",
            "           1       0.57      0.51      0.54       281\n",
            "\n",
            "    accuracy                           0.77      1057\n",
            "   macro avg       0.70      0.68      0.69      1057\n",
            "weighted avg       0.76      0.77      0.76      1057\n",
            "\n",
            "LR AUC-ROC: 0.7889\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../models/saved_churn_lr.pkl']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr.fit(Xc_train_s, yc_train)\n",
        "pred_lr = lr.predict(Xc_test_s)\n",
        "prob_lr = lr.predict_proba(Xc_test_s)[:,1]\n",
        "print('Logistic Regression Classification Report:')\n",
        "print(classification_report(yc_test, pred_lr))\n",
        "print(f'LR AUC-ROC: {roc_auc_score(yc_test, prob_lr):.4f}')\n",
        "joblib.dump(lr, '../models/saved_churn_lr.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03524203",
      "metadata": {},
      "source": [
        "## 7. Train Sentiment Classification Models\n",
        "\n",
        "Train SVM and LSTM models on the preprocessed sentiment data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b2c3d4",
      "metadata": {},
      "source": [
        "### 7.1 Multinomial Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e0db511",
      "metadata": {},
      "source": [
        "### 7.2 SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "1e559e5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.67      0.70     12306\n",
            "     neutral       0.59      0.11      0.19      6396\n",
            "    positive       0.89      0.97      0.93     66567\n",
            "\n",
            "    accuracy                           0.86     85269\n",
            "   macro avg       0.73      0.58      0.60     85269\n",
            "weighted avg       0.84      0.86      0.84     85269\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../models/saved_tfidf.pkl']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_s = tfidf.fit_transform(train_s['clean_text'])\n",
        "X_test_s  = tfidf.transform(test_s['clean_text'])\n",
        "\n",
        "le_s = LabelEncoder()\n",
        "y_train_s = le_s.fit_transform(train_s['label'])\n",
        "y_test_s  = le_s.transform(test_s['label'])\n",
        "\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train_s, y_train_s)\n",
        "\n",
        "y_pred_s = svm.predict(X_test_s)\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test_s, y_pred_s, target_names=le_s.classes_))\n",
        "\n",
        "joblib.dump(svm, '../models/saved_svm.pkl')\n",
        "joblib.dump(tfidf, '../models/saved_tfidf.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece2ee33",
      "metadata": {},
      "source": [
        "### 7.3 LSTM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "8eebf5f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shahriarpshuvo/Academic/research-methodology-code/.venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m12435/12435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 34ms/step - accuracy: 0.8092 - loss: 0.5645 - val_accuracy: 0.8720 - val_loss: 0.3489\n",
            "Epoch 2/5\n",
            "\u001b[1m12435/12435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 36ms/step - accuracy: 0.8828 - loss: 0.3207 - val_accuracy: 0.8826 - val_loss: 0.3267\n",
            "Epoch 3/5\n",
            "\u001b[1m12435/12435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 35ms/step - accuracy: 0.9054 - loss: 0.2643 - val_accuracy: 0.8921 - val_loss: 0.3050\n",
            "Epoch 4/5\n",
            "\u001b[1m12435/12435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 33ms/step - accuracy: 0.9202 - loss: 0.2269 - val_accuracy: 0.8933 - val_loss: 0.3122\n",
            "Epoch 5/5\n",
            "\u001b[1m12435/12435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 33ms/step - accuracy: 0.9338 - loss: 0.1912 - val_accuracy: 0.8970 - val_loss: 0.3138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM Test Accuracy: 0.8981\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../models/saved_tokenizer.pkl']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_s['clean_text'])\n",
        "\n",
        "def to_padded_seqs(df):\n",
        "    seqs = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "    return pad_sequences(seqs, maxlen=100, padding='post')\n",
        "\n",
        "X_tr = to_padded_seqs(train_s)\n",
        "X_val = to_padded_seqs(val_s)\n",
        "X_te = to_padded_seqs(test_s)\n",
        "\n",
        "le_l = LabelEncoder()\n",
        "y_tr = to_categorical(le_l.fit_transform(train_s['label']))\n",
        "y_val = to_categorical(le_l.transform(val_s['label']))\n",
        "y_te = to_categorical(le_l.transform(test_s['label']))\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=100),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.5),\n",
        "    LSTM(32),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_tr, y_tr, validation_data=(X_val, y_val), epochs=5, batch_size=32)\n",
        "\n",
        "loss, acc = model.evaluate(X_te, y_te, verbose=0)\n",
        "print(f\"LSTM Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "model.save('../models/saved_lstm.h5')\n",
        "joblib.dump(tokenizer, '../models/saved_tokenizer.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e5f6g7h8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.82      0.26      0.40     12306\n",
            "     neutral       0.47      0.00      0.00      6396\n",
            "    positive       0.81      0.99      0.90     66567\n",
            "\n",
            "    accuracy                           0.81     85269\n",
            "   macro avg       0.70      0.42      0.43     85269\n",
            "weighted avg       0.79      0.81      0.76     85269\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['../models/saved_mnb.pkl']"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Multinomial Naive Bayes\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_s, y_train_s)\n",
        "\n",
        "y_pred_mnb = mnb.predict(X_test_s)\n",
        "print(\"Multinomial Naive Bayes Classification Report:\")\n",
        "print(classification_report(y_test_s, y_pred_mnb, target_names=le_s.classes_))\n",
        "\n",
        "joblib.dump(mnb, '../models/saved_mnb.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bb8d9b1",
      "metadata": {},
      "source": [
        "## 8. Evaluate Models\n",
        "\n",
        "Evaluate trained models on held‑out test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "6831b6dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### SVM Evaluation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.67      0.70     12306\n",
            "     neutral       0.59      0.11      0.19      6396\n",
            "    positive       0.89      0.97      0.93     66567\n",
            "\n",
            "    accuracy                           0.86     85269\n",
            "   macro avg       0.73      0.58      0.60     85269\n",
            "weighted avg       0.84      0.86      0.84     85269\n",
            "\n",
            "\n",
            "### Multinomial Naive Bayes Evaluation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.82      0.26      0.40     12306\n",
            "     neutral       0.47      0.00      0.00      6396\n",
            "    positive       0.81      0.99      0.90     66567\n",
            "\n",
            "    accuracy                           0.81     85269\n",
            "   macro avg       0.70      0.42      0.43     85269\n",
            "weighted avg       0.79      0.81      0.76     85269\n",
            "\n",
            "\n",
            "### LSTM Evaluation\n",
            "LSTM Test Accuracy: 0.8981\n",
            "\n",
            "### Logistic Regression Evaluation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       776\n",
            "           1       0.57      0.51      0.54       281\n",
            "\n",
            "    accuracy                           0.77      1057\n",
            "   macro avg       0.70      0.68      0.69      1057\n",
            "weighted avg       0.76      0.77      0.76      1057\n",
            "\n",
            "LR AUC-ROC: 0.7889\n",
            "\n",
            "### Random Forest Evaluation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       776\n",
            "           1       0.66      0.46      0.54       281\n",
            "\n",
            "    accuracy                           0.79      1057\n",
            "   macro avg       0.74      0.69      0.70      1057\n",
            "weighted avg       0.78      0.79      0.78      1057\n",
            "\n",
            "RF AUC-ROC: 0.8160\n"
          ]
        }
      ],
      "source": [
        "# Sentiment Model Evaluation\n",
        "print(\"### SVM Evaluation\")\n",
        "print(classification_report(y_test_s, svm.predict(X_test_s), target_names=le_s.classes_))\n",
        "\n",
        "print(\"\\n### Multinomial Naive Bayes Evaluation\")\n",
        "print(classification_report(y_test_s, mnb.predict(X_test_s), target_names=le_s.classes_))\n",
        "\n",
        "print(\"\\n### LSTM Evaluation\")\n",
        "loss, acc = model.evaluate(X_te, y_te, verbose=0)\n",
        "print(f\"LSTM Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Churn Model Evaluation\n",
        "print(\"\\n### Logistic Regression Evaluation\")\n",
        "print(classification_report(yc_test, lr.predict(Xc_test_s)))\n",
        "print(f\"LR AUC-ROC: {roc_auc_score(yc_test, prob_lr):.4f}\")\n",
        "\n",
        "print(\"\\n### Random Forest Evaluation\")\n",
        "print(classification_report(yc_test, rf.predict(Xc_test_s)))\n",
        "print(f\"RF AUC-ROC: {roc_auc_score(yc_test, prob_rf):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
